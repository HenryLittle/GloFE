{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('/mnt/workspace/How2Sign/how2sign_realigned_train.csv', sep='\\t')\n",
    "\n",
    "# Load training text samples\n",
    "# split = 'train'\n",
    "column = 'SENTENCE'\n",
    "# data_frame = data_frame.loc[data_frame['split'].str.contains(split)]\n",
    "translation = data_frame[column]\n",
    "vids = data_frame['SENTENCE_NAME']\n",
    "\n",
    "# Using Punkt to tokenize words\n",
    "sent_tks = [word_tokenize(s.lower()) for s in translation]\n",
    "tag_res = [nltk.pos_tag(tks) for tks in sent_tks]\n",
    "\n",
    "joined_tag_res = []\n",
    "for l in tag_res:\n",
    "    joined_tag_res.extend(l)\n",
    "\n",
    "freq_dist = nltk.ConditionalFreqDist(joined_tag_res)\n",
    "\n",
    "exclude_words = ['was', 'i', 'said', 'aslcaptions.com', '\\'s', 'is', 'be', 'are', 'has', 'www.aslcaptions.com', 'did', '\\'ve', '\\'m', '%', 've', 'r', 'd', '*', 'b', 'ed', 'e.', '[', ']', 'dpan.tv', 'iii', '<', '>', '/i', '\\'re', '']\n",
    "collect_keys = {'NN', 'NNP', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
    "filtered_words = {}\n",
    "for word, freq in freq_dist.items():\n",
    "    if word in exclude_words: continue\n",
    "    key_set = set(freq.keys())\n",
    "    intersect = collect_keys.intersection(key_set)\n",
    "    if len(intersect) > 0:\n",
    "        filtered_freq = {}\n",
    "        for tag in intersect:\n",
    "            if freq[tag] > 10:\n",
    "                filtered_freq[tag] = freq[tag]\n",
    "        if len(filtered_freq) > 0: \n",
    "            filtered_words[word] = filtered_freq\n",
    "\n",
    "# Load GloVe embeddings\n",
    "# vocab = []\n",
    "# embeddings = []\n",
    "# with open('/mnt/workspace/slt_baseline/notebooks/glove/glove.6B.300d.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         items = line.strip().split(' ')\n",
    "#         vocab.append(items[0])\n",
    "#         embeddings.append(np.asarray(items[1:], 'float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('how2sign/uncased_filtred_VNs.json', 'w') as f:\n",
    "    json.dump(filtered_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cross filter with glove vocabulary\n",
    "import numpy as np\n",
    "vocab = []\n",
    "embeddings = []\n",
    "with open('/mnt/workspace/slt_baseline/notebooks/glove/glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        items = line.strip().split(' ')\n",
    "        vocab.append(items[0])\n",
    "        embeddings.append(np.asarray(items[1:], 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_dict = json.load(open('how2sign/uncased_filtred_VNs.json', 'r'))\n",
    "VNs = VN_dict.keys()\n",
    "\n",
    "OOV = 0\n",
    "OOV_word = []\n",
    "for vn in VNs:\n",
    "    if vn not in vocab:\n",
    "        OOV += 1\n",
    "        OOV_word.append(vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 o.k\n",
      "11 itâs\n",
      "19 donât\n",
      "12 iâm\n",
      "14 thatâs\n",
      "11 solutionz\n"
     ]
    }
   ],
   "source": [
    "for k in OOV_word:\n",
    "    stat = VN_dict[k]\n",
    "    total = 0\n",
    "    for pos, num in stat.items():\n",
    "        total += num\n",
    "    print(total, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in OOV_word:\n",
    "    VN_dict.pop(k)\n",
    "\n",
    "with open('how2sign/uncased_filtred_glove_VNs.json', 'w') as f:\n",
    "    json.dump(VN_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Index word mapping for glove filtered VNs\n",
    "import json\n",
    "\n",
    "with open('how2sign/uncased_filtred_glove_VNs.json', 'r') as f:\n",
    "    vn_dict = json.load(f)\n",
    "\n",
    "vn_words = list(vn_dict.keys())\n",
    "with open('how2sign/uncased_filtred_glove_VN_idxs.txt', 'w') as f:\n",
    "    for idx, word in enumerate(vn_words):\n",
    "        f.write(f'{idx} {word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the corresponding embedding pkl\n",
    "import numpy as np\n",
    "\n",
    "vn_glove_embeddings = []\n",
    "\n",
    "glove_embedding_dict = {}\n",
    "\n",
    "with open('/mnt/workspace/slt_baseline/notebooks/glove/glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        items = line.strip().split(' ')\n",
    "        glove_embedding_dict[items[0]] = np.asarray(items[1:], 'float32')\n",
    "        \n",
    "for word in vn_words:\n",
    "    vn_glove_embeddings.append(glove_embedding_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2191, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn_glove_embed = np.stack(vn_glove_embeddings, axis=0)\n",
    "vn_glove_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('how2sign/uncased_filtred_glove_VN_embed.pkl', 'wb') as f:\n",
    "    pkl.dump(vn_glove_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('/mnt/workspace/How2Sign/how2sign_realigned_train.csv', sep='\\t')\n",
    "\n",
    "# Load training text samples\n",
    "# split = 'train'\n",
    "column = 'SENTENCE'\n",
    "# data_frame = data_frame.loc[data_frame['split'].str.contains(split)]\n",
    "translation = data_frame[column]\n",
    "vids = data_frame['SENTENCE_NAME']\n",
    "\n",
    "VN_dict = json.load(open('how2sign/uncased_filtred_glove_VNs.json', 'r'))\n",
    "VNs = VN_dict.keys()\n",
    "\n",
    "matched = {}\n",
    "for vid, trans in zip(vids, translation):\n",
    "    ref_word_list = word_tokenize(trans)\n",
    "    matched_words = []\n",
    "    for ref_word in ref_word_list:\n",
    "        if ref_word in VNs:\n",
    "            matched_words.append(ref_word)\n",
    "    matched[vid] = matched_words\n",
    "\n",
    "with open('how2sign/uncased_filtred_glove_VN_matched_train.json', 'w') as f:\n",
    "    json.dump(matched, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a5ee8f268a58a1501ad7aef09cde53105f57cea18e29cd62af7d0e62261f331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
