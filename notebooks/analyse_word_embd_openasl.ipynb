{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/slt/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_frame = pd.read_csv('/mnt/workspace/openasl-pre/openasl-v1.0.tsv', sep='\\t')\n",
    "\n",
    "# Load training text samples\n",
    "split = 'train'\n",
    "column = 'raw-text'\n",
    "data_frame = data_frame.loc[data_frame['split'].str.contains(split)]\n",
    "translation = data_frame[column]\n",
    "vids = data_frame['vid']\n",
    "\n",
    "# Using Punkt to tokenize words\n",
    "sent_tks = [word_tokenize(s.lower()) for s in translation]\n",
    "tag_res = [nltk.pos_tag(tks) for tks in sent_tks]\n",
    "\n",
    "joined_tag_res = []\n",
    "for l in tag_res:\n",
    "    joined_tag_res.extend(l)\n",
    "\n",
    "freq_dist = nltk.ConditionalFreqDist(joined_tag_res)\n",
    "\n",
    "exclude_words = ['was', 'i', 'said', 'aslcaptions.com', '\\'s', 'is', 'be', 'are', 'has', 'www.aslcaptions.com', 'did', '\\'ve', '\\'m', '%', 've', 'r', 'd', '*', 'b', 'ed', 'e.', '[', ']', 'dpan.tv', 'iii', '<', '>', '/i', '']\n",
    "collect_keys = {'NN', 'NNP', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
    "filtered_words = {}\n",
    "for word, freq in freq_dist.items():\n",
    "    if word in exclude_words: continue\n",
    "    key_set = set(freq.keys())\n",
    "    intersect = collect_keys.intersection(key_set)\n",
    "    if len(intersect) > 0:\n",
    "        filtered_freq = {}\n",
    "        for tag in intersect:\n",
    "            if freq[tag] > 10:\n",
    "                filtered_freq[tag] = freq[tag]\n",
    "        if len(filtered_freq) > 0: \n",
    "            filtered_words[word] = filtered_freq\n",
    "\n",
    "# Load GloVe embeddings\n",
    "# vocab = []\n",
    "# embeddings = []\n",
    "# with open('/mnt/workspace/slt_baseline/notebooks/glove/glove.6B.300d.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         items = line.strip().split(' ')\n",
    "#         vocab.append(items[0])\n",
    "#         embeddings.append(np.asarray(items[1:], 'float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('openasl-v1.0/uncased_filtred_VNs.json', 'w') as f:\n",
    "    json.dump(filtered_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cross filter with glove vocabulary\n",
    "import numpy as np\n",
    "vocab = []\n",
    "embeddings = []\n",
    "with open('/mnt/workspace/slt_baseline/notebooks/glove/glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        items = line.strip().split(' ')\n",
    "        vocab.append(items[0])\n",
    "        embeddings.append(np.asarray(items[1:], 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_dict = json.load(open('openasl-v1.0/uncased_filtred_VNs.json', 'r'))\n",
    "VNs = VN_dict.keys()\n",
    "\n",
    "OOV = 0\n",
    "OOV_word = []\n",
    "for vn in VNs:\n",
    "    if vn not in vocab:\n",
    "        OOV += 1\n",
    "        OOV_word.append(vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 guyger\n",
      "15 renca\n",
      "13 taloali\n",
      "47 convo\n",
      "285 mavrick\n",
      "17 dcara\n",
      "49 vaping\n",
      "18 vindman\n",
      "357 covid-19\n",
      "21 sdsd\n",
      "88 ylc\n",
      "50 gofundme\n",
      "33 breonna\n",
      "23 hydroxychloroquine\n",
      "13 yovanovitch\n",
      "20 dpan\n",
      "33 ntid\n",
      "13 ntid/rit\n",
      "23 burisma\n",
      "57 cordano\n",
      "12 aasd\n",
      "35 ehdi\n",
      "15 deafverse\n",
      "44 covid\n",
      "17 ocasio-cortez\n",
      "14 guaido\n",
      "13 hcq\n",
      "13 h.w\n",
      "18 fsdb\n",
      "36 audism\n",
      "21 brexit\n",
      "25 feimer\n",
      "50 sondland\n",
      "38 zelensky\n",
      "12 besgrove\n",
      "12 retweeted\n",
      "26 selfie\n",
      "55 arbery\n",
      "12 bipoc\n",
      "28 nbda\n",
      "25 tlaib\n",
      "29 bolsonaro\n",
      "11 faceapp\n",
      "12 captioner\n",
      "11 rayshard\n",
      "11 emoji\n",
      "27 tiktok\n",
      "2401 sign1news\n",
      "515 wooddall\n",
      "23 drejka\n",
      "26 selfies\n",
      "16 e-cigarettes\n",
      "17 berreth\n",
      "15 emojis\n",
      "12 ochoa-lopez\n",
      "12 blasey\n",
      "12 kaytee\n",
      "14 deblasio\n",
      "11 sayoc\n",
      "34 maleah\n",
      "17 remdesivir\n",
      "11 mssd\n",
      "12 daca\n",
      "11 asdc\n",
      "12 paffumi\n",
      "14 deaf/hard\n",
      "12 d'oeil\n",
      "11 fortnite\n",
      "11 deafspace\n",
      "20 dwu\n",
      "25 kpta\n",
      "11 vallow\n",
      "11 captioners\n",
      "15 ip-cts\n",
      "23 nltc\n",
      "11 re-authorization\n",
      "16 nbsp\n",
      "24 nad.org\n",
      "32 h.r\n",
      "14 dejoy\n",
      "39 crpd\n",
      "12 schnatter\n"
     ]
    }
   ],
   "source": [
    "for k in OOV_word:\n",
    "    stat = VN_dict[k]\n",
    "    total = 0\n",
    "    for pos, num in stat.items():\n",
    "        total += num\n",
    "    print(total, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in OOV_word:\n",
    "    VN_dict.pop(k)\n",
    "\n",
    "with open('openasl-v1.0/uncased_filtred_glove_VNs.json', 'w') as f:\n",
    "    json.dump(VN_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Index word mapping for glove filtered VNs\n",
    "import json\n",
    "\n",
    "with open('openasl-v1.0/uncased_filtred_glove_VNs.json', 'r') as f:\n",
    "    vn_dict = json.load(f)\n",
    "\n",
    "vn_words = list(vn_dict.keys())\n",
    "with open('openasl-v1.0/uncased_filtred_glove_VN_idxs.txt', 'w') as f:\n",
    "    for idx, word in enumerate(vn_words):\n",
    "        f.write(f'{idx} {word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the corresponding embedding pkl\n",
    "import numpy as np\n",
    "\n",
    "vn_glove_embeddings = []\n",
    "\n",
    "glove_embedding_dict = {}\n",
    "\n",
    "with open('/mnt/workspace/slt_baseline/notebooks/glove/glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        items = line.strip().split(' ')\n",
    "        glove_embedding_dict[items[0]] = np.asarray(items[1:], 'float32')\n",
    "        \n",
    "for word in vn_words:\n",
    "    vn_glove_embeddings.append(glove_embedding_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5523, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn_glove_embed = np.stack(vn_glove_embeddings, axis=0)\n",
    "vn_glove_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('openasl-v1.0/uncased_filtred_glove_VN_embed.pkl', 'wb') as f:\n",
    "    pkl.dump(vn_glove_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trainning infomation\n",
    "data_frame = pd.read_csv('/mnt/workspace/openasl-pre/openasl-v1.0.tsv', sep='\\t')\n",
    "\n",
    "# Load training text samples\n",
    "split = 'train'\n",
    "column = 'raw-text'\n",
    "data_frame = data_frame.loc[data_frame['split'].str.contains(split)]\n",
    "translation = data_frame[column]\n",
    "vids = data_frame['vid']\n",
    "\n",
    "VN_dict = json.load(open('openasl-v1.0/uncased_filtred_glove_VNs.json', 'r'))\n",
    "VNs = VN_dict.keys()\n",
    "\n",
    "matched = {}\n",
    "for vid, trans in zip(vids, translation):\n",
    "    ref_word_list = word_tokenize(trans)\n",
    "    matched_words = []\n",
    "    for ref_word in ref_word_list:\n",
    "        if ref_word in VNs:\n",
    "            matched_words.append(ref_word)\n",
    "    matched[vid] = matched_words\n",
    "\n",
    "with open('openasl-v1.0/uncased_filtred_glove_VN_matched_train.json', 'w') as f:\n",
    "    json.dump(matched, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a5ee8f268a58a1501ad7aef09cde53105f57cea18e29cd62af7d0e62261f331"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
