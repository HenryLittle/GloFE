{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import torch \n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/mnt/workspace/slt_baseline/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def import_class(name):\n",
    "    components = name.split('.')\n",
    "    mod = __import__(components[0])\n",
    "    for comp in components[1:]:\n",
    "        mod = getattr(mod, comp)\n",
    "    return mod\n",
    "\n",
    "\n",
    "def conv_branch_init(conv, branches):\n",
    "    weight = conv.weight\n",
    "    n = weight.size(0)\n",
    "    k1 = weight.size(1)\n",
    "    k2 = weight.size(2)\n",
    "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
    "    nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "\n",
    "def conv_init(conv):\n",
    "    if conv.weight is not None:\n",
    "        nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
    "    if conv.bias is not None:\n",
    "        nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "\n",
    "def bn_init(bn, scale):\n",
    "    nn.init.constant_(bn.weight, scale)\n",
    "    nn.init.constant_(bn.bias, 0)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        if hasattr(m, 'bias') and m.bias is not None and isinstance(m.bias, torch.Tensor):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        pad = (kernel_size + (kernel_size-1) * (dilation-1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiScale_TemporalConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 dilations=[1,2,3,4],\n",
    "                 residual=True,\n",
    "                 residual_kernel_size=1):\n",
    "\n",
    "        super().__init__()\n",
    "        assert out_channels % (len(dilations) + 2) == 0, '# out channels should be multiples of # branches'\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "        if type(kernel_size) == list:\n",
    "            assert len(kernel_size) == len(dilations)\n",
    "        else:\n",
    "            kernel_size = [kernel_size]*len(dilations)\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                TemporalConv(\n",
    "                    branch_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=ks,\n",
    "                    stride=stride,\n",
    "                    dilation=dilation),\n",
    "            )\n",
    "            for ks, dilation in zip(kernel_size, dilations)\n",
    "        ])\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(3,1), stride=(stride,1), padding=(1,0)),\n",
    "            nn.BatchNorm2d(branch_channels)  # 为什么还要加bn\n",
    "        ))\n",
    "\n",
    "        self.branches.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0, stride=(stride,1)),\n",
    "            nn.BatchNorm2d(branch_channels)\n",
    "        ))\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(in_channels, out_channels, kernel_size=residual_kernel_size, stride=stride)\n",
    "\n",
    "        # initialize\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        res = self.residual(x)\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "\n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        out += res\n",
    "        return out\n",
    "\n",
    "\n",
    "class CTRGC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rel_reduction=8, mid_reduction=1):\n",
    "        super(CTRGC, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        if in_channels == 3 or in_channels == 5 or in_channels == 9:\n",
    "            self.rel_channels = 8\n",
    "            self.mid_channels = 16\n",
    "        else:\n",
    "            self.rel_channels = in_channels // rel_reduction\n",
    "            self.mid_channels = in_channels // mid_reduction\n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.rel_channels, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(self.in_channels, self.rel_channels, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1)\n",
    "        self.conv4 = nn.Conv2d(self.rel_channels, self.out_channels, kernel_size=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                conv_init(m)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                bn_init(m, 1)\n",
    "\n",
    "    def forward(self, x, A=None, alpha=1):\n",
    "        # x: [16, 3, 52, 55] => [B, C, T, V]\n",
    "        # x1, x2: [16, 8, 55]  -> mapping 'Theta' and 'Phi'\n",
    "        # x3: [16, 64, 52, 55] -> Feature Transformation\n",
    "        x1, x2, x3 = self.conv1(x).mean(-2), self.conv2(x).mean(-2), self.conv3(x)\n",
    "        # M \n",
    "        x1 = self.tanh(x1.unsqueeze(-1) - x2.unsqueeze(-2))\n",
    "        # R\n",
    "        x1 = self.conv4(x1) * alpha + (A.unsqueeze(0).unsqueeze(0) if A is not None else 0)  # N,C,V,V\n",
    "        x1 = torch.einsum('ncuv,nctv->nctu', x1, x3)\n",
    "        return x1\n",
    "\n",
    "class unit_tcn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1):\n",
    "        super(unit_tcn, self).__init__()\n",
    "        pad = int((kernel_size - 1) / 2)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
    "                              stride=(stride, 1))\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        conv_init(self.conv)\n",
    "        bn_init(self.bn, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.conv(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class unit_gcn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, A, coff_embedding=4, adaptive=True, residual=True):\n",
    "        super(unit_gcn, self).__init__()\n",
    "        inter_channels = out_channels // coff_embedding\n",
    "        self.inter_c = inter_channels\n",
    "        self.out_c = out_channels\n",
    "        self.in_c = in_channels\n",
    "        self.adaptive = adaptive\n",
    "        self.num_subset = A.shape[0]\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(self.num_subset):\n",
    "            self.convs.append(CTRGC(in_channels, out_channels))\n",
    "\n",
    "        if residual:\n",
    "            if in_channels != out_channels:\n",
    "                self.down = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, 1),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "            else:\n",
    "                self.down = lambda x: x\n",
    "        else:\n",
    "            self.down = lambda x: 0\n",
    "        if self.adaptive: # wether to update the Adj Matrix\n",
    "            self.PA = nn.Parameter(torch.from_numpy(A.astype(np.float32)))\n",
    "        else:\n",
    "            self.A = Variable(torch.from_numpy(A.astype(np.float32)), requires_grad=False)\n",
    "        self.alpha = nn.Parameter(torch.zeros(1)) # zero gradient\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.soft = nn.Softmax(-2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                conv_init(m)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                bn_init(m, 1)\n",
    "        bn_init(self.bn, 1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = None\n",
    "        if self.adaptive:\n",
    "            A = self.PA\n",
    "        else:\n",
    "            A = self.A.cuda(x.get_device())\n",
    "        for i in range(self.num_subset):\n",
    "            z = self.convs[i](x, A[i], self.alpha)\n",
    "\n",
    "            y = z + y if y is not None else z\n",
    "        y = self.bn(y)\n",
    "        y += self.down(x)\n",
    "        y = self.relu(y)\n",
    "\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class TCN_GCN_unit(nn.Module):\n",
    "    \"\"\"\n",
    "        A single stack of spatial GCN and temporal TCN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, A, stride=1, residual=True, adaptive=True, kernel_size=5, dilations=[1,2]):\n",
    "        super(TCN_GCN_unit, self).__init__()\n",
    "        self.gcn1 = unit_gcn(in_channels, out_channels, A, adaptive=adaptive)\n",
    "        self.tcn1 = MultiScale_TemporalConv(out_channels, out_channels, kernel_size=kernel_size, stride=stride, dilations=dilations,\n",
    "                                            residual=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "\n",
    "        else:\n",
    "            self.residual = unit_tcn(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.tcn1(self.gcn1(x)) + self.residual(x))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_class=60, num_point=25, num_person=2, graph=None, graph_args=dict(), in_channels=3,\n",
    "                 drop_out=0, adaptive=True):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        if graph is None:\n",
    "            raise ValueError()\n",
    "        else:\n",
    "            Graph = import_class(graph)\n",
    "            self.graph = Graph(**graph_args)\n",
    "\n",
    "        A = self.graph.A # 3,25,25\n",
    "\n",
    "        self.num_class = num_class\n",
    "        self.num_point = num_point\n",
    "        self.data_bn = nn.BatchNorm1d(num_person * in_channels * num_point)\n",
    "\n",
    "        base_channel = 64\n",
    "        self.l1 = TCN_GCN_unit(in_channels, base_channel, A, residual=False, adaptive=adaptive)\n",
    "        self.l2 = TCN_GCN_unit(base_channel, base_channel, A, adaptive=adaptive)\n",
    "        self.l3 = TCN_GCN_unit(base_channel, base_channel, A, adaptive=adaptive)\n",
    "        self.l4 = TCN_GCN_unit(base_channel, base_channel, A, adaptive=adaptive)\n",
    "        self.l5 = TCN_GCN_unit(base_channel, base_channel*2, A, stride=2, adaptive=adaptive)\n",
    "        self.l6 = TCN_GCN_unit(base_channel*2, base_channel*2, A, adaptive=adaptive)\n",
    "        self.l7 = TCN_GCN_unit(base_channel*2, base_channel*2, A, adaptive=adaptive)\n",
    "        self.l8 = TCN_GCN_unit(base_channel*2, base_channel*4, A, stride=2, adaptive=adaptive)\n",
    "        self.l9 = TCN_GCN_unit(base_channel*4, base_channel*4, A, adaptive=adaptive)\n",
    "        self.l10 = TCN_GCN_unit(base_channel*4, base_channel*4, A, adaptive=adaptive)\n",
    "\n",
    "        self.fc = nn.Linear(base_channel*4, num_class)\n",
    "        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n",
    "        bn_init(self.data_bn, 1)\n",
    "        if drop_out:\n",
    "            self.drop_out = nn.Dropout(drop_out)\n",
    "        else:\n",
    "            self.drop_out = lambda x: x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [16, 3, 52, 55, 1] => [B, C, T, V, Man]\n",
    "        # if len(x.shape) == 3:\n",
    "        #     N, T, VC = x.shape\n",
    "        #     x = x.view(N, T, self.num_point, -1).permute(0, 3, 1, 2).contiguous().unsqueeze(-1)\n",
    "        # N, C, T, V, M = x.size()\n",
    "\n",
    "        # x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n",
    "        # # x = self.data_bn(x)\n",
    "        # x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)\n",
    "        \n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.l6(x)\n",
    "        x = self.l7(x)\n",
    "        x = self.l8(x)\n",
    "        x = self.l9(x)\n",
    "        x = self.l10(x)\n",
    "\n",
    "        # N*M,C,T,V \n",
    "        # [16, 256, 13, 55]\n",
    "        # c_new = x.size(1)\n",
    "        # x = x.view(N, M, c_new, -1)\n",
    "        # x = x.mean(3).mean(1)\n",
    "        # x = self.drop_out(x)\n",
    "\n",
    "        # return self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseBackboneWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseBackboneWrapper, self).__init__()\n",
    "        self.pose_model = Model(\n",
    "                num_class=2000, num_point=78, num_person=1, \n",
    "                graph='models.graph.openpose_78.Graph',\n",
    "                graph_args={'labeling_mode': 'spatial'}, drop_out=0)\n",
    "        pose_weights = torch.load('/mnt/workspace/slt_baseline/models/ckpt/ctr_op78_mix_HF05_F64_e1/runs-82-93316.pt')\n",
    "        # output [B C T V]\n",
    "        self.pose_model.load_state_dict(pose_weights, strict=True)\n",
    "    \n",
    "    def forward(self, prefix):\n",
    "        pose_output = self.pose_model(prefix) # B C T V\n",
    "        pose_pool = pose_output.mean(-1).mean(-1) # B C T\n",
    "        # prefix = pose_pool.transpose(-1, -2) # B T C\n",
    "        out_cls = self.pose_model.fc(pose_pool) # B T Class\n",
    "        return out_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_slide(length, span=8, step=2):\n",
    "    if length <= span:\n",
    "        diff = span - length\n",
    "        idxs = np.array(range(length))\n",
    "        idxs = np.concatenate((idxs, (length-1)*np.ones(diff)))\n",
    "        idxs = idxs[None,:]\n",
    "    else:\n",
    "        num_clips = (length - span + (step - 1)) // step + 1\n",
    "        offsets = np.arange(num_clips)[:,None] * step\n",
    "        idxs = offsets + np.arange(span)[None, :]\n",
    "    # idxs = np.mod(idxs, length) # ensure no out of bounds\n",
    "    idxs = idxs.clip(max=length-1) \n",
    "    return idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_pose_file(filepath):\n",
    "    # exclude lower body parts    \n",
    "\n",
    "    body_pose_exclude = {9, 10, 11, 22, 23, 24, 12, 13, 14, 19, 20, 21}\n",
    "\n",
    "    body_sample_indices = [x for x in range(25) if x not in body_pose_exclude]\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        pose_dict = pkl.load(f)\n",
    "    # [X Y Confidence]\n",
    "    body_pose = pose_dict['pose_keypoints'] # [F, 25, 3] -> [F, 13, 2]\n",
    "    hand_left = pose_dict['hand_left_keypoints'] # [F, 21, 3] -> [F, 21, 2]\n",
    "    hand_right = pose_dict['hand_right_keypoints'] # [F, 21, 3] -> [F, 21, 2]\n",
    "    face = pose_dict['face_keypoints'] #  [F, 70, 3] -> [F, 70, 2]\n",
    "    # sample body\n",
    "    body_pose = body_pose[:, body_sample_indices, :] # 23(17+6) 11 selected\n",
    "    # sample face\n",
    "    face_sample_index = [71, 77, 85, 89] + \\\n",
    "                        [40, 42, 44, 45, 47, 49] + \\\n",
    "                        [59, 60, 61, 62, 63, 64] + [65, 66, 67, 68, 69, 70] + \\\n",
    "                        [50]\n",
    "    face_sample_index = [(x - 23) for x in face_sample_index]\n",
    "    face = face[:, face_sample_index, :] # 23 Keypoints\n",
    "\n",
    "    pose_tuple = (body_pose, hand_left, hand_right, face)\n",
    "    pose_cated = np.concatenate(pose_tuple, axis=1) # [F, 13+21+21+23=78, 3]\n",
    "\n",
    "    pose_cated[:, :, 0:2] = 2.0 * ((pose_cated[:, :, 0:2] / 256.0) - 0.5) # scale to [-1, 1] by image frame\n",
    "\n",
    "    return pose_cated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _normalize_joints(value):\n",
    "    # scale to [-1, 1]\n",
    "    scalerValue = np.reshape(value, (-1, 3))\n",
    "    scalerValue = (scalerValue - np.min(scalerValue, axis=0)) / ((np.max(scalerValue, axis=0) - np.min(scalerValue,axis=0)) + 1e-5)\n",
    "        \n",
    "    scalerValue = scalerValue * 2 - 1\n",
    "    scalerValue = np.reshape(scalerValue, (-1, 78, 3))\n",
    "\n",
    "    return scalerValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(input_data, model):\n",
    "    device = 'cuda:0'\n",
    "    model = model.to(device)\n",
    "    input_data = torch.from_numpy(input_data).type(torch.float32).to(device)\n",
    "    input_data = input_data.permute(0, 3, 1, 2) # B T V C -> B C T V\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data) # B T Class\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_openpose_sample(sample_name, wlasl_openpose_root='/mnt/workspace/WLASL/data/openpose'):\n",
    "    file_path = os.path.join(wlasl_openpose_root, f'{sample_name}', f'{sample_name}.pkl')\n",
    "    value = read_pose_file(file_path)\n",
    "    value[:,:,:2] = _normalize_joints(value)[:,:,:2]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "class_path = '/mnt/workspace/CTR-GCN/wlasl2000_label.txt'\n",
    "with open(class_path, 'r') as f:\n",
    "    vocab = f.readlines()\n",
    "    vocab = [x.strip() for x in vocab]\n",
    "print(len(vocab))\n",
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import json\n",
    "subset = 'asl2000'\n",
    "split = 'test'\n",
    "\n",
    "wlasl_root = '/mnt/workspace/WLASL'\n",
    "split_file = os.path.join(wlasl_root, f'data/splits/{subset}.json')\n",
    "label_encoder = LabelEncoder()\n",
    "with open(split_file, 'r') as f:\n",
    "    content = json.load(f)\n",
    "# init lable encoder\n",
    "glosses = sorted([gloss_entry['gloss'] for gloss_entry in content])\n",
    "label_encoder.fit(glosses)\n",
    "# \n",
    "data_list = []\n",
    "for entry in content:\n",
    "    gloss, instances = entry['gloss'], entry['instances']\n",
    "    gloss_cat = label_encoder.transform([gloss])[0] # label index\n",
    "    for instance in instances:\n",
    "        if instance['split'] not in split:\n",
    "            continue\n",
    "\n",
    "        video_id = instance['video_id']\n",
    "        \n",
    "        instance_entry = video_id, gloss_cat\n",
    "        data_list.append(instance_entry)\n",
    "train_dict = dict(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 78, 3)\n"
     ]
    }
   ],
   "source": [
    "sample = '57488' # label 963\n",
    "lable = train_dict[sample]\n",
    "value = load_openpose_sample(sample)\n",
    "print(value.shape)\n",
    "T, V, C = value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([12, 2000])\n",
      "Index shape: ()\n",
      "Target: terrible\n",
      "cool\n"
     ]
    }
   ],
   "source": [
    "# sliding window inference\n",
    "slide_window = gen_slide(T, span=8, step=2)\n",
    "input_data = value[slide_window, ...]\n",
    "output = inference(value[slide_window, ...], model) # B T Class\n",
    "print('Output shape:', output.shape)\n",
    "output = output.mean(-2)\n",
    "idxs = output.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "print('Index shape:', idxs.shape)\n",
    "slide_out = vocab[idxs]\n",
    "print('Target:', vocab[lable])\n",
    "print(slide_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2000])\n",
      "Index shape: ()\n",
      "Target: terrible\n",
      "before\n"
     ]
    }
   ],
   "source": [
    "sample_idx = np.linspace(0, T-1, 52).astype(np.int)\n",
    "output = inference(value[None, sample_idx, ...], model)\n",
    "# output = inference(value[None, ...], model)\n",
    "print('Output shape:', output.shape)\n",
    "output = output.mean(-2)\n",
    "idxs = output.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "print('Index shape:', idxs.shape)\n",
    "slide_out = vocab[idxs]\n",
    "print('Target:', vocab[lable])\n",
    "print(slide_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc88a9a29c042949bb8f1eea944b7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2879 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-788bb75bd064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m52\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# output = output.mean(-2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6635c907cfb9>\u001b[0m in \u001b[0;36minference\u001b[0;34m(input_data, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B T V C -> B C T V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B T Class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b26fcb76894a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prefix)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpose_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B C T V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpose_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B C T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# prefix = pose_pool.transpose(-1, -2) # B T C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0dd733db884d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0dd733db884d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0dd733db884d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mbranch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtempconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mbranch_outs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "model = PoseBackboneWrapper()\n",
    "\n",
    "total = len(train_dict)\n",
    "print(total)\n",
    "count = 0\n",
    "curlen = 0\n",
    "progress = tqdm(total=total)\n",
    "for s, l in train_dict.items():\n",
    "    value = load_openpose_sample(s)\n",
    "    T, V, C = value.shape\n",
    "    sample_idx = np.linspace(0, T-1, 52).astype(np.int)\n",
    "    output = inference(value[None, sample_idx, :, :], model)\n",
    "    # output = output.mean(-2)\n",
    "    idxs = output.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "    # print(idxs)\n",
    "    curlen += 1\n",
    "    if idxs == l:\n",
    "        count += 1\n",
    "    progress.set_postfix({'acc': count/curlen})\n",
    "    progress.update()\n",
    "progress.close()\n",
    "print(f'{count}/{total}, {count/total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'07092': 210, '07093': 210, '07095': 210, '07072': 210, '17730': 568, '17713': 568, '17716': 568, '17718': 568, '12306': 392, '12332': 392, '12336': 392, '12320': 392, '67519': 392, '05735': 168, '05727': 168, '05739': 168, '05741': 168, '09847': 313, '09867': 313, '09855': 313, '24857': 789, '24960': 789, '67715': 789, '11305': 358, '11311': 358, '11318': 358, '63219': 1955, '63226': 1955, '63233': 1955, '65300': 285, '08915': 285, '08925': 285, '65415': 433, '13635': 433, '67535': 433, '14855': 472, '14883': 472, '14888': 472, '65717': 703, '70234': 703, '21890': 703, '27194': 858, '27209': 858, '27213': 858, '38482': 1166, '38525': 1166, '38531': 1166, '57941': 1797, '57943': 1797, '57944': 1797, '62152': 1913, '62160': 1913, '62166': 1913, '64201': 1991, '64210': 1991, '64209': 1991, '64275': 1993, '64293': 1993, '64297': 1993, '68720': 48, '01986': 48, '01996': 48, '06455': 196, '06481': 196, '06483': 196, '70271': 418, '13198': 418, '13196': 418, '21933': 705, '70361': 705, '21944': 705, '28074': 884, '28119': 884, '28121': 884, '69389': 1026, '68093': 1026, '33286': 1026, '34822': 1063, '34834': 1063, '34836': 1063, '36927': 1118, '67908': 1118, '36940': 1118, '38982': 1178, '39005': 1178, '38991': 1178, '40114': 1212, '67949': 1212, '40126': 1212, '56552': 1744, '67275': 1744, '56567': 1744, '57646': 1782, '57637': 1782, '57640': 1782, '62967': 1946, '62987': 1946, '62975': 1946, '63662': 1972, '63664': 1972, '63675': 1972, '05628': 164, '05639': 164, '06822': 204, '06845': 204, '07383': 222, '07400': 222, '69257': 280, '08947': 280, '17097': 548, '65506': 548, '67578': 548, '20976': 678, '20991': 678, '22109': 709, '22117': 709, '22126': 709, '25318': 802, '67722': 802, '26688': 840, '26723': 840, '65884': 849, '26984': 849, '31750': 977, '31758': 977, '31749': 977, '32157': 987, '32166': 987, '32167': 987, '67829': 994, '32329': 994, '34685': 1060, '34743': 1060, '51077': 1553, '51066': 1553, '51068': 1553, '55356': 1704, '55375': 1704, '56835': 1751, '56843': 1751, '67278': 1751, '63191': 1954, '63201': 1954, '64082': 1990, '64097': 1990, '00625': 8, '00634': 8, '00635': 8, '02997': 81, '03004': 81, '06326': 191, '06335': 191, '09945': 317, '09950': 317, '11752': 371, '11772': 371, '13325': 422, '67530': 422, '13334': 422, '13681': 435, '13708': 435, '13710': 435, '14621': 462, '14626': 462, '14673': 466, '14682': 466, '14685': 466, '17020': 546, '67579': 546, '17024': 546, '18316': 596, '18326': 596, '19255': 626, '19262': 626, '65761': 732, '22954': 732, '24636': 784, '24649': 784, '32253': 990, '32246': 990, '32258': 990, '35506': 1078, '35513': 1078, '68132': 1284, '42840': 1284, '42842': 1284, '42953': 1286, '69431': 1286, '42960': 1286, '69433': 1291, '43171': 1291, '49577': 1509, '68145': 1509, '50041': 1523, '67177': 1523, '50050': 1523, '51233': 1559, '51234': 1559, '58488': 1813, '58502': 1813, '62241': 1917, '62247': 1917, '63769': 1977, '63789': 1977, '01386': 31, '01394': 31, '01397': 31, '05227': 154, '69225': 154, '06366': 193, '06370': 193, '06360': 193, '07957': 247, '69252': 247, '08429': 260, '08436': 260, '08425': 260, '10156': 323, '10159': 323, '10888': 347, '69269': 347, '13154': 416, '70030': 416, '15031': 477, '70119': 477, '70029': 754, '23767': 754, '28187': 888, '69370': 888, '28201': 888, '30830': 950, '68320': 950, '68636': 1016, '70325': 1016, '32950': 1016, '66111': 1077, '68810': 1077, '37879': 1147, '70237': 1147, '40816': 1229, '68364': 1229, '41008': 1232, '68446': 1232, '45252': 1374, '69439': 1374, '45432': 1379, '69440': 1379, '48126': 1465, '48115': 1465, '49173': 1497, '49179': 1497, '49180': 1497, '53258': 1634, '68336': 1634, '57273': 1764, '57278': 1764, '58359': 1809, '70026': 1809, '61804': 1899, '68248': 1899, '62077': 1910, '62113': 1910, '62479': 1926, '68179': 1926, '63325': 1958, '70333': 1958, '64260': 1992, '69545': 1992, '04614': 137, '68956': 137, '65123': 137, '65137': 148, '05088': 148, '05089': 148, '07929': 246, '69251': 246, '09431': 299, '69261': 299, '10186': 324, '10195': 324, '10196': 324, '10965': 348, '69270': 348, '14172': 447, '14190': 447, '16190': 518, '68034': 518, '17317': 554, '70264': 554, '25674': 814, '69353': 814, '26122': 828, '68424': 828, '26757': 842, '69360': 842, '26832': 845, '26835': 845, '29654': 919, '29657': 919, '29660': 919, '31841': 980, '70320': 980, '32379': 995, '67831': 995, '32389': 995, '32598': 1005, '68088': 1005, '37120': 1126, '68104': 1126, '45835': 1397, '45837': 1397, '45846': 1397, '46260': 1407, '68400': 1407, '46712': 1419, '69446': 1419, '48507': 1477, '48510': 1477, '48514': 1477, '48797': 1487, '68762': 1487, '66473': 1564, '70231': 1564, '51494': 1568, '69470': 1568, '52834': 1616, '69481': 1616, '56701': 1745, '56693': 1745, '56695': 1745, '57034': 1754, '68412': 1754, '57037': 1756, '68166': 1756, '62728': 1936, '68740': 1936, '63279': 1956, '70213': 1956, '63574': 1967, '69539': 1967, '64049': 1989, '69544': 1989, '70347': 1994, '64304': 1994, '01457': 34, '68276': 34, '04694': 139, '69219': 139, '04833': 143, '69221': 143, '05296': 156, '05299': 156, '06550': 198, '06553': 198, '08478': 264, '69255': 264, '08706': 274, '08701': 274, '11552': 367, '70298': 367, '65377': 368, '11638': 368, '11704': 370, '68026': 370, '13258': 421, '68756': 421, '14450': 457, '68652': 457, '14748': 468, '70334': 468, '20067': 650, '20077': 650, '21052': 681, '21078': 681, '65682': 681, '22071': 708, '22096': 708, '23567': 747, '68214': 747, '25072': 796, '25070': 796, '26506': 837, '26525': 837, '27762': 874, '69366': 874, '31896': 982, '31900': 982, '32298': 993, '70297': 993, '32675': 1006, '66025': 1006, '33450': 1033, '33475': 1033, '34004': 1046, '34016': 1046, '37575': 1139, '69405': 1139, '39604': 1199, '69419': 1199, '42196': 1264, '68646': 1264, '43519': 1300, '70198': 1300, '44677': 1351, '44684': 1351, '47174': 1434, '68141': 1434, '50846': 1547, '50853': 1547, '53290': 1636, '53296': 1636, '53292': 1636, '54548': 1676, '54555': 1676, '55768': 1715, '70022': 1715, '57519': 1774, '57530': 1774, '58588': 1815, '68488': 1815, '59203': 1843, '59206': 1843, '59479': 1851, '68478': 1851, '63415': 1963, '68184': 1963, '64351': 1995, '69547': 1995, '67339': 4, '00424': 4, '03125': 86, '03120': 86, '03445': 96, '03436': 96, '04805': 141, '04796': 141, '04905': 145, '04896': 145, '67397': 160, '05477': 160, '05605': 162, '05607': 162, '07462': 225, '07454': 225, '08377': 258, '08379': 258, '09188': 294, '09180': 294, '09729': 309, '09731': 309, '10111': 321, '10102': 321, '67495': 333, '10472': 333, '10696': 343, '10708': 343, '11206': 355, '11209': 355, '11262': 356, '11265': 356, '13135': 415, '13148': 415, '13553': 431, '13555': 431, '69284': 438, '13801': 438, '14802': 470, '14793': 470, '16595': 535, '16597': 535, '17665': 567, '17656': 567, '17751': 569, '17768': 569, '65544': 570, '17822': 570, '21239': 685, '21241': 685, '21442': 694, '21434': 694, '22810': 727, '22802': 727, '23955': 760, '23946': 760, '24026': 764, '24028': 764, '24606': 782, '24608': 782, '25251': 800, '25253': 800, '26946': 848, '26955': 848, '27263': 860, '27273': 860, '27922': 880, '27933': 880, '28160': 887, '28162': 887, '28473': 897, '28465': 897, '30153': 929, '30155': 929, '65981': 958, '31159': 958, '68090': 1024, '33206': 1024, '33537': 1036, '33545': 1036, '34585': 1059, '34577': 1059, '35305': 1072, '35289': 1072, '36838': 1114, '36840': 1114, '36914': 1117, '36916': 1117, '37373': 1133, '37365': 1133, '38133': 1156, '38124': 1156, '38587': 1167, '38589': 1167, '68826': 1196, '39457': 1196, '66248': 1213, '40184': 1213, '66261': 1231, '40994': 1231, '41316': 1239, '41325': 1239, '41457': 1241, '41448': 1241, '41832': 1254, '41823': 1254, '43067': 1288, '43058': 1288, '43219': 1293, '43221': 1293, '44081': 1320, '44083': 1320, '44377': 1335, '44368': 1335, '47666': 1454, '47657': 1454, '48053': 1463, '48055': 1463, '48905': 1489, '48913': 1489, '49139': 1495, '49130': 1495, '49254': 1498, '49245': 1498, '51614': 1570, '51633': 1570, '67204': 1578, '51774': 1578, '52560': 1606, '52564': 1606, '53209': 1628, '53211': 1628, '53493': 1643, '53484': 1643, '69494': 1703, '55348': 1703, '57059': 1755, '57061': 1755, '57792': 1790, '57794': 1790, '58794': 1826, '58785': 1826, '59317': 1846, '59319': 1846, '60352': 1872, '60348': 1872, '62283': 1918, '62274': 1918, '67061': 1950, '63082': 1950, '64439': 1997, '64428': 1997, '02229': 58, '02231': 58, '02585': 68, '02587': 68, '03278': 90, '03281': 90, '04507': 133, '04509': 133, '04600': 135, '04593': 135, '67382': 140, '04770': 140, '05276': 155, '05278': 155, '05813': 172, '05815': 172, '07809': 243, '07811': 243, '09468': 300, '09470': 300, '69264': 310, '09774': 310, '09915': 315, '09917': 315, '10268': 326, '10270': 326, '13468': 427, '13470': 427, '13855': 440, '13857': 440, '15332': 486, '15321': 486, '15372': 487, '15374': 487, '16449': 530, '16438': 530, '67577': 544, '16975': 544, '17605': 564, '17595': 564, '18289': 593, '18291': 593, '18307': 595, '18309': 595, '18497': 601, '18486': 601, '19410': 631, '19413': 631, '21204': 686, '21217': 686, '21281': 687, '21286': 687, '22549': 720, '22551': 720, '23581': 748, '23583': 748, '67710': 787, '24732': 787, '26250': 831, '26258': 831, '26568': 838, '26570': 838, '27013': 851, '27047': 851, '28148': 886, '28150': 886, '28318': 891, '28308': 891, '28428': 895, '28429': 895, '29137': 910, '29139': 910, '30242': 932, '30232': 932, '31317': 964, '31320': 964, '31659': 974, '31650': 974, '32449': 997, '32451': 997, '35122': 1066, '35114': 1066, '35362': 1075, '35364': 1075, '36056': 1094, '36047': 1094, '36648': 1107, '36658': 1107, '36735': 1110, '36737': 1110, '36790': 1112, '36780': 1112, '37085': 1125, '37097': 1125, '37785': 1143, '37787': 1143, '37986': 1151, '37988': 1151, '38283': 1160, '38285': 1160, '38317': 1162, '38319': 1162, '38622': 1168, '38624': 1168, '38683': 1170, '38685': 1170, '38859': 1173, '38861': 1173, '39088': 1182, '39090': 1182, '39451': 1194, '39447': 1194, '39584': 1198, '39592': 1198, '41513': 1243, '41507': 1243, '41601': 1245, '41604': 1245, '42254': 1266, '42256': 1266, '43931': 1314, '43933': 1314, '48982': 1491, '48972': 1491, '49328': 1500, '49330': 1500, '49380': 1503, '49372': 1503, '49653': 1512, '49655': 1512, '50064': 1522, '50066': 1522, '51129': 1555, '51131': 1555, '68600': 1557, '51181': 1557, '51703': 1573, '51705': 1573, '51893': 1583, '51883': 1583, '52319': 1601, '52321': 1601, '53375': 1638, '53367': 1638, '55055': 1690, '55044': 1690, '56307': 1734, '56309': 1734, '56819': 1750, '67277': 1750, '57313': 1765, '57302': 1765, '57438': 1772, '57440': 1772, '57667': 1780, '57657': 1780, '57975': 1799, '57977': 1799, '68560': 1808, '58295': 1808, '58679': 1821, '58681': 1821, '59254': 1845, '59246': 1845, '60547': 1876, '60548': 1876, '61067': 1885, '61070': 1885, '61914': 1904, '61916': 1904, '61989': 1907, '61978': 1907, '61992': 1908, '61994': 1908, '62707': 1935, '62709': 1935, '62908': 1943, '62910': 1943, '62923': 1944, '62925': 1944, '63076': 1949, '63067': 1949, '63106': 1951, '63108': 1951, '63470': 1961, '63472': 1961, '63753': 1976, '63755': 1976, '01436': 33, '01438': 33, '01487': 36, '01489': 36, '02105': 54, '02107': 54, '03058': 82, '03060': 82, '04195': 118, '04198': 118, '04346': 125, '04348': 125, '04869': 144, '04871': 144, '05195': 152, '05197': 152, '05495': 159, '05497': 159, '05855': 174, '05857': 174, '06070': 183, '06072': 183, '06652': 201, '06654': 201, '07202': 214, '07192': 214, '07372': 221, '07374': 221, '07420': 223, '07422': 223, '07472': 228, '07474': 228, '07599': 233, '07601': 233, '08590': 268, '08592': 268, '09245': 290, '09249': 290, '10408': 331, '10411': 331, '10446': 332, '10448': 332, '10639': 339, '10641': 339, '10772': 344, '10774': 344, '13174': 417, '13177': 417, '14289': 450, '14291': 450, '14485': 456, '14473': 456, '15100': 478, '65451': 478, '15160': 481, '15162': 481, '15175': 482, '15177': 482, '15534': 492, '15536': 492, '16407': 528, '16409': 528, '16896': 541, '16899': 541, '17432': 559, '17434': 559, '17934': 574, '17936': 574, '18156': 587, '18158': 587, '18198': 589, '18200': 589, '18225': 590, '67605': 590, '18772': 609, '18774': 609, '19235': 625, '19237': 625, '19559': 635, '19561': 635, '20365': 659, '20367': 659, '20438': 663, '20440': 663, '65691': 684, '21177': 684, '21704': 698, '21707': 698, '21853': 702, '21855': 702, '22134': 710, '22136': 710, '22492': 718, '22494': 718, '22642': 722, '22633': 722, '22718': 724, '22720': 724, '23641': 750, '23643': 750, '24440': 777, '24443': 777, '26498': 836, '26500': 836, '27172': 856, '27175': 856, '27592': 870, '27594': 870, '28034': 882, '28036': 882, '28649': 900, '28651': 900, '29071': 908, '29073': 908, '30469': 940, '30471': 940, '30937': 953, '30933': 953, '30987': 954, '30974': 954, '31732': 976, '31734': 976, '31776': 978, '31780': 978, '32206': 989, '32208': 989, '32283': 992, '32285': 992, '32812': 1010, '32813': 1010, '32965': 1017, '32967': 1017, '34959': 1065, '34961': 1065, '35528': 1079, '35530': 1079, '66128': 1098, '36249': 1098, '36262': 1099, '36264': 1099, '66131': 1100, '36337': 1100, '36680': 1108, '36682': 1108, '36859': 1115, '36861': 1115, '36995': 1121, '36997': 1121, '37851': 1146, '37854': 1146, '38087': 1155, '38089': 1155, '38182': 1158, '38184': 1158, '38338': 1163, '38341': 1163, '39849': 1204, '39851': 1204, '41924': 1257, '41927': 1257, '41938': 1258, '41940': 1258, '42426': 1270, '42428': 1270, '42655': 1278, '42657': 1278, '43341': 1295, '43343': 1295, '43449': 1298, '43452': 1298, '43681': 1306, '43683': 1306, '43829': 1311, '43831': 1311, '45758': 1393, '45760': 1393, '45996': 1402, '45998': 1402, '66374': 1408, '46300': 1408, '47501': 1447, '47493': 1447, '49479': 1507, '49481': 1507, '49635': 1510, '49637': 1510, '49693': 1514, '49699': 1514, '50407': 1533, '50416': 1533, '50837': 1546, '50840': 1546, '51918': 1584, '51920': 1584, '52509': 1605, '52511': 1605, '52714': 1613, '52716': 1613, '52920': 1618, '52922': 1618, '53027': 1622, '53029': 1622, '53701': 1650, '53696': 1650, '54440': 1672, '54442': 1672, '54823': 1683, '54825': 1683, '55302': 1700, '55304': 1700, '55316': 1701, '55318': 1701, '55857': 1717, '55859': 1717, '56083': 1725, '56085': 1725, '57827': 1792, '57829': 1792, '58447': 1812, '58449': 1812, '58654': 1819, '58658': 1819, '58734': 1823, '58736': 1823, '58808': 1827, '58810': 1827, '59143': 1842, '59154': 1842, '59368': 1848, '59370': 1848, '59549': 1852, '59551': 1852, '59760': 1857, '59770': 1857, '60461': 1874, '60463': 1874, '61145': 1888, '61147': 1888, '62642': 1933, '67049': 1933, '62733': 1937, '62735': 1937, '00601': 7, '00602': 7, '01194': 25, '01215': 25, '01421': 32, '01422': 32, '01554': 38, '01555': 38, '02048': 52, '02049': 52, '02353': 60, '02354': 60, '02544': 65, '02545': 65, '02715': 73, '02716': 73, '02831': 74, '02832': 74, '65091': 89, '68812': 89, '03481': 98, '03483': 98, '03640': 103, '03641': 103, '04173': 117, '04174': 117, '04428': 130, '04429': 130, '04441': 131, '04442': 131, '04683': 138, '04684': 138, '05108': 150, '05109': 150, '05654': 165, '05655': 165, '05708': 167, '05709': 167, '05927': 177, '05928': 177, '06194': 187, '06195': 187, '06431': 195, '06432': 195, '07275': 217, '07276': 217, '69256': 269, '08620': 269, '08673': 273, '08674': 273, '08845': 281, '08846': 281, '09308': 297, '09309': 297, '09576': 304, '09577': 304, '09896': 314, '09897': 314, '10139': 322, '10140': 322, '10583': 335, '10584': 335, '11039': 350, '11040': 350, '11364': 359, '67506': 359, '12118': 386, '12119': 386, '12226': 390, '12227': 390, '12875': 407, '12876': 407, '12972': 409, '12973': 409, '13317': 423, '13318': 423, '13357': 424, '13358': 424, '14382': 453, '14383': 453, '14839': 471, '14840': 471, '15426': 489, '15427': 489, '15606': 496, '15607': 496, '65467': 501, '15730': 501, '15778': 503, '15779': 503, '15937': 509, '15938': 509, '16128': 516, '16129': 516, '17146': 549, '17149': 549, '18063': 581, '18064': 581, '18283': 594, '18272': 594, '18523': 602, '18524': 602, '18801': 610, '18790': 610, '18880': 612, '18882': 612, '19114': 618, '19115': 618, '19331': 629, '19332': 629, '19688': 638, '19689': 638, '19752': 641, '19769': 641, '20141': 652, '20142': 652, '20382': 660, '20383': 660, '20700': 670, '20701': 670, '20798': 673, '20799': 673, '21414': 692, '21415': 692, '21552': 697, '21554': 697, '22276': 713, '22277': 713, '22748': 725, '22749': 725, '22997': 728, '22998': 728, '23042': 735, '23050': 735, '23447': 743, '23448': 743, '23543': 746, '23544': 746, '23668': 752, '23669': 752, '24585': 781, '24586': 781, '24918': 792, '24919': 792, '25375': 804, '25376': 804, '25410': 805, '25411': 805, '25637': 812, '25622': 812, '25646': 813, '25647': 813, '25924': 819, '25925': 819, '25980': 822, '25981': 822, '26326': 833, '26313': 833, '27155': 855, '27156': 855, '27412': 863, '27414': 863, '27571': 869, '27572': 869, '28380': 892, '28381': 892, '28453': 896, '28454': 896, '30388': 938, '30389': 938, '30454': 939, '30455': 939, '32514': 999, '32515': 999, '33056': 1020, '33057': 1020, '33125': 1022, '33126': 1022, '33643': 1038, '33644': 1038, '34134': 1049, '34136': 1049, '34206': 1050, '34207': 1050, '34390': 1055, '34391': 1055, '35343': 1074, '35344': 1074, '35946': 1090, '35947': 1090, '37474': 1136, '37475': 1136, '37960': 1150, '37961': 1150, '39038': 1179, '39039': 1179, '39748': 1203, '39749': 1203, '40463': 1218, '40468': 1218, '41667': 1249, '41668': 1249, '42706': 1281, '42707': 1281, '43575': 1303, '43576': 1303, '43900': 1313, '43909': 1313, '44034': 1319, '66319': 1319, '44351': 1333, '44334': 1333, '44469': 1339, '66330': 1339, '44606': 1349, '44607': 1349, '45483': 1382, '45484': 1382, '45857': 1398, '45858': 1398, '46373': 1411, '46374': 1411, '46432': 1412, '46433': 1412, '47108': 1432, '47109': 1432, '47606': 1450, '47609': 1450, '48752': 1486, '48740': 1486, '49081': 1493, '49082': 1493, '49542': 1508, '49543': 1508, '50272': 1528, '50273': 1528, '50506': 1537, '50507': 1537, '51330': 1565, '51331': 1565, '51816': 1581, '51817': 1581, '52588': 1607, '52589': 1607, '52617': 1609, '52618': 1609, '53070': 1624, '53071': 1624, '53418': 1640, '53419': 1640, '53432': 1641, '53433': 1641, '53551': 1645, '53553': 1645, '54076': 1659, '54077': 1659, '54209': 1665, '54210': 1665, '54882': 1686, '54883': 1686, '54999': 1689, '55000': 1689, '55074': 1691, '55075': 1691, '55154': 1694, '55155': 1694, '55180': 1695, '55181': 1695, '55202': 1697, '55203': 1697, '55279': 1698, '55280': 1698, '55663': 1710, '55664': 1710, '55735': 1712, '55736': 1712, '56162': 1728, '56163': 1728, '56299': 1735, '56300': 1735, '57136': 1758, '57137': 1758, '57679': 1783, '57680': 1783, '57762': 1788, '57763': 1788, '57804': 1791, '57805': 1791, '58028': 1801, '58029': 1801, '58392': 1810, '58393': 1810, '58939': 1832, '58940': 1832, '59905': 1862, '59906': 1862, '59965': 1863, '59966': 1863, '60764': 1880, '60765': 1880, '60855': 1881, '60856': 1881, '62458': 1925, '62447': 1925, '62590': 1931, '62591': 1931, '62693': 1934, '62694': 1934, '63372': 1959, '63374': 1959, '63407': 1962, '63408': 1962, '63831': 1980, '63832': 1980, '63875': 1982, '63876': 1982, '63961': 1985, '63962': 1985, '64416': 1996, '64417': 1996, '69205': 18, '00973': 18, '01721': 44, '67354': 44, '02158': 56, '02161': 56, '02173': 57, '02174': 57, '02473': 62, '02475': 62, '02517': 64, '02518': 64, '02564': 67, '67364': 67, '02941': 79, '02961': 79, '03328': 92, '67369': 92, '03360': 93, '03362': 93, '03420': 95, '03425': 95, '65100': 101, '03563': 101, '03587': 102, '03603': 102, '04039': 109, '04042': 109, '04056': 111, '04067': 111, '04814': 142, '04819': 142, '65138': 149, '05074': 149, '05165': 151, '05171': 151, '05756': 169, '05761': 169, '05959': 178, '05961': 178, '06407': 194, '06419': 194, '06526': 197, '06530': 197, '06929': 206, '06930': 206, '06941': 207, '06949': 207, '07226': 215, '07228': 215, '07236': 216, '07246': 216, '07259': 218, '07261': 218, '07284': 219, '07287': 219, '70166': 220, '07306': 220, '07490': 229, '67437': 229, '08118': 251, '08121': 251, '08330': 256, '08355': 256, '08439': 261, '08449': 261, '08739': 275, '08747': 275, '08858': 282, '08861': 282, '08897': 284, '08899': 284, '09149': 292, '65305': 292, '10588': 336, '10591': 336, '11812': 372, '11820': 372, '11900': 375, '11905': 375, '65394': 396, '12587': 396, '13654': 434, '13658': 434, '15556': 494, '15558': 494, '15807': 504, '15808': 504, '15825': 505, '15827': 505, '16061': 512, '16069': 512, '17123': 550, '17135': 550, '17588': 565, '17582': 565, '17624': 566, '65534': 566, '17907': 573, '17911': 573, '17945': 575, '67600': 575, '18027': 579, '18031': 579, '18137': 586, '18138': 586, '18164': 588, '18169': 588, '19456': 632, '19458': 632, '19798': 642, '19844': 642, '20152': 653, '20162': 653, '65657': 655, '67636': 655, '20387': 661, '20390': 661, '20402': 662, '20414': 662, '20736': 671, '20747': 671, '21319': 689, '21325': 689, '21394': 693, '21405': 693, '22702': 723, '22695': 723, '22907': 730, '22912': 730, '23326': 740, '23328': 740, '23626': 749, '23628': 749, '23865': 758, '23869': 758, '24056': 766, '24058': 766, '24867': 790, '24878': 790, '25003': 794, '25012': 794, '25024': 795, '25033': 795, '25182': 799, '25185': 799, '25428': 807, '25429': 807, '25481': 808, '25483': 808, '25829': 816, '25830': 816, '65865': 821, '67733': 821, '26294': 832, '26297': 832, '26697': 841, '26708': 841, '26831': 844, '26855': 844, '27281': 859, '27274': 859, '27983': 881, '67772': 881, '28772': 903, '28775': 903, '29087': 909, '29088': 909, '29171': 911, '29721': 911, '29380': 914, '29385': 914, '29865': 923, '29868': 923, '30051': 928, '30054': 928, '30674': 946, '30676': 946, '32692': 1007, '32695': 1007, '32885': 1014, '32887': 1014, '33303': 1027, '33313': 1027, '33390': 1030, '33393': 1030, '33462': 1034, '33463': 1034, '34868': 1064, '34871': 1064, '35147': 1067, '67876': 1067, '69397': 1070, '35219': 1070, '35371': 1076, '67884': 1076, '69399': 1105, '36606': 1105, '37040': 1123, '67911': 1123, '37201': 1127, '37195': 1127, '37313': 1130, '37315': 1130, '37332': 1131, '37333': 1131, '37395': 1134, '37429': 1134, '37396': 1135, '67919': 1135, '37909': 1148, '37912': 1148, '38008': 1152, '38021': 1152, '38061': 1154, '38062': 1154, '38721': 1172, '38730': 1172, '39503': 1197, '39514': 1197, '39984': 1207, '39995': 1207, '40010': 1208, '40021': 1208, '40308': 1215, '40315': 1215, '66267': 1240, '41360': 1240, '42752': 1282, '42756': 1282, '44267': 1331, '44275': 1331, '44422': 1337, '44424': 1337, '44491': 1341, '44494': 1341, '44828': 1357, '44831': 1357, '44896': 1360, '44898': 1360, '45494': 1383, '45514': 1383, '45684': 1390, '45686': 1390, '45777': 1394, '45783': 1394, '45905': 1400, '45909': 1400, '46155': 1405, '46176': 1405, '46997': 1426, '47000': 1426, '47458': 1445, '47463': 1445, '47668': 1453, '47671': 1453, '47782': 1457, '47788': 1457, '48207': 1467, '48214': 1467, '48249': 1468, '48252': 1468, '48603': 1482, '48608': 1482, '50107': 1524, '50122': 1524, '50138': 1525, '50141': 1525, '50328': 1530, '50341': 1530, '50719': 1543, '67182': 1543, '50932': 1550, '50936': 1550, '51028': 1552, '51029': 1552, '51094': 1554, '51112': 1554, '51272': 1560, '51290': 1560, '51686': 1572, '51689': 1572, '52168': 1595, '52179': 1595, '52197': 1597, '52198': 1597, '52236': 1599, '52228': 1599, '53435': 1642, '53446': 1642, '53640': 1648, '53643': 1648, '53899': 1656, '53904': 1656, '54255': 1667, '54258': 1667, '54360': 1670, '54377': 1670, '54586': 1677, '54598': 1677, '54915': 1687, '54926': 1687, '70371': 1705, '67257': 1705, '55745': 1713, '55748': 1713, '55837': 1716, '55847': 1716, '56191': 1730, '56194': 1730, '57260': 1763, '57262': 1763, '58629': 1818, '58724': 1818, '58705': 1822, '58708': 1822, '59404': 1850, '59405': 1850, '59596': 1853, '59607': 1853, '60043': 1865, '60046': 1865, '66705': 1877, '67019': 1877, '60887': 1882, '60909': 1882, '60947': 1883, '60949': 1883, '61737': 1898, '61739': 1898, '61965': 1906, '61968': 1906, '62285': 1919, '62291': 1919, '62301': 1920, '62303': 1920, '62329': 1921, '62356': 1921, '63447': 1964, '63450': 1964, '63621': 1970, '63626': 1970, '63720': 1975, '63723': 1975, '64543': 1999, '67017': 1999, '00838': 11, '00903': 16, '01581': 39, '01805': 45, '01959': 50, '02097': 53, '02624': 70, '02879': 77, '04016': 108, '04116': 113, '04160': 116, '04295': 122, '04393': 129, '05564': 161, '05621': 163, '05917': 176, '06028': 181, '06051': 182, '06164': 186, '06318': 190, '07450': 226, '07635': 234, '07756': 240, '08000': 248, '08134': 252, '08415': 259, '08786': 277, '08822': 278, '09093': 289, '09214': 293, '09322': 298, '09639': 306, '09811': 311, '09841': 312, '09936': 316, '10017': 319, '10095': 320, '11014': 349, '11065': 351, '11145': 354, '11947': 377, '12067': 383, '12197': 388, '12296': 391, '13019': 411, '13252': 420, '13418': 425, '13463': 426, '13750': 437, '14318': 451, '14372': 452, '14571': 459, '15550': 493, '15712': 500, '16086': 513, '16103': 514, '16435': 529, '16810': 539, '16928': 542, '17157': 551, '17383': 557, '17394': 558, '17993': 578, '18247': 591, '18258': 592, '18577': 603, '18901': 613, '18983': 615, '19093': 617, '19171': 619, '19310': 628, '19708': 639, '19974': 647, '20013': 648, '20029': 649, '20341': 658, '21027': 680, '21304': 688, '21835': 700, '23278': 739, '23485': 744, '23705': 753, '23809': 756, '65803': 769, '24175': 769, '24382': 776, '24827': 788, '24892': 791, '25049': 797, '25850': 817, '26008': 823, '26220': 830, '26799': 843, '26801': 843, '27321': 861, '27314': 861, '27448': 865, '27801': 875, '27879': 878, '28589': 899, '28737': 902, '29333': 913, '29405': 915, '29638': 920, '30269': 933, '30318': 937, '30515': 941, '30709': 947, '31056': 955, '31487': 969, '31560': 971, '32119': 986, '32480': 998, '32538': 1000, '32866': 1012, '33340': 1028, '33699': 1040, '34091': 1047, '34116': 1048, '34270': 1051, '34358': 1053, '34371': 1054, '35610': 1083, '36236': 1097, '36633': 1106, '37023': 1122, '37067': 1124, '37348': 1132, '37616': 1140, '37843': 1145, '38233': 1159, '39353': 1189, '39956': 1206, '40641': 1222, '40656': 1223, '40764': 1227, '40808': 1228, '41069': 1234, '41170': 1237, '42001': 1260, '42090': 1262, '42400': 1269, '42477': 1271, '42992': 1287, '43558': 1302, '43732': 1308, '44130': 1322, '44206': 1328, '44325': 1334, '44571': 1346, '44565': 1347, '44631': 1350, '44740': 1353, '44865': 1359, '45066': 1366, '45105': 1368, '45116': 1369, '45178': 1371, '45368': 1376, '45576': 1385, '45721': 1391, '45747': 1392, '45974': 1403, '46059': 1404, '46964': 1425, '47243': 1436, '47862': 1460, '47913': 1461, '48022': 1462, '48494': 1476, '48544': 1479, '48719': 1484, '49435': 1505, '49650': 1511, '49732': 1515, '49808': 1517, '50021': 1521, '50323': 1529, '50437': 1534, '50554': 1539, '51396': 1566, '51480': 1567, '51550': 1569, '51939': 1585, '52295': 1600, '52944': 1619, '53017': 1621, '53159': 1626, '53246': 1632, '53830': 1653, '53880': 1655, '53874': 1655, '54176': 1663, '54299': 1668, '54431': 1673, '54462': 1674, '54958': 1688, '55123': 1692, '55294': 1699, '55677': 1711, '55807': 1714, '55983': 1720, '56002': 1721, '56035': 1722, '56110': 1726, '56252': 1731, '56347': 1736, '56465': 1741, '56977': 1753, '57096': 1757, '57242': 1761, '57416': 1771, '57731': 1785, '57776': 1789, '57842': 1794, '66608': 1798, '57924': 1798, '58008': 1800, '58138': 1803, '58187': 1804, '58427': 1811, '58952': 1833, '59057': 1837, '59109': 1839, '59851': 1861, '60226': 1869, '60305': 1870, '60306': 1870, '60384': 1873, '61370': 1893, '61881': 1903, '62427': 1924, '62779': 1930, '62622': 1932, '67048': 1932, '62760': 1938, '62834': 1942, '63501': 1965, '63612': 1969, '63658': 1971, '63855': 1981, '64047': 1988, '64448': 1998, '00434': 5, '00667': 9, '01161': 24, '01248': 27, '02149': 55, '02615': 69, '02701': 72, '03107': 85, '03454': 97, '03521': 100, '04582': 136, '05686': 166, '05797': 171, '05880': 175, '06373': 192, '06630': 200, '06737': 202, '07173': 213, '07579': 232, '07681': 237, '07782': 242, '08173': 253, '08446': 262, '09128': 291, '09494': 301, '09522': 303, '09741': 308, '10314': 328, '10343': 329, '10519': 334, '10648': 340, '10692': 342, '10791': 345, '11095': 352, '11383': 360, '11443': 363, '11475': 364, '11503': 365, '11533': 366, '12019': 381, '12078': 384, '13048': 412, '13535': 430, '14267': 449, '14583': 460, '15218': 484, '15296': 485, '15480': 491, '15578': 495, '16164': 517, '16237': 520, '16328': 523, '16336': 524, '16357': 525, '16388': 526, '16530': 532, '16568': 534, '17303': 552, '17899': 572, '18115': 585, '18421': 598, '18725': 606, '19054': 616, '19224': 624, '19621': 636, '20202': 654, '20624': 667, '20771': 672, '20930': 676, '21014': 679, '21096': 682, '21491': 695, '22240': 712, '22415': 715, '22460': 716, '22486': 717, '23008': 734, '23204': 737, '23855': 755, '23846': 757, '23913': 759, '23992': 762, '24256': 771, '24520': 779, '24686': 785, '24735': 786, '25171': 798, '25575': 811, '25947': 820, '26088': 827, '26919': 846, '26964': 850, '27081': 853, '27101': 854, '27191': 857, '27381': 862, '27440': 864, '27638': 871, '27673': 872, '27828': 876, '28266': 889, '28394': 893, '28880': 906, '29620': 918, '30173': 930, '30261': 934, '30566': 943, '30745': 948, '31097': 957, '31252': 962, '31567': 970, '31981': 983, '32558': 1002, '32735': 1008, '32829': 1011, '32883': 1013, '33379': 1029, '33770': 1041, '33811': 1042, '34401': 1056, '34515': 1057, '34549': 1058, '34704': 1062, '35215': 1069, '35549': 1071, '35881': 1087, '35970': 1091, '36123': 1095, '36420': 1102, '36719': 1109, '37270': 1128, '37489': 1137, '38843': 1175, '39366': 1190, '39384': 1192, '39431': 1195, '39844': 1201, '39880': 1205, '40055': 1209, '40398': 1217, '40582': 1221, '41486': 1242, '41981': 1259, '42558': 1274, '42570': 1275, '42588': 1276, '42676': 1279, '43117': 1290, '43386': 1297, '43702': 1307, '43813': 1310, '44237': 1330, '44447': 1338, '44955': 1362, '45329': 1375, '45454': 1380, '45872': 1399, '46333': 1410, '46595': 1416, '46869': 1423, '47042': 1428, '47306': 1438, '47336': 1439, '47363': 1441, '47386': 1442, '47456': 1444, '47643': 1451, '47735': 1456, '47841': 1459, '48175': 1466, '48330': 1471, '48376': 1474, '48409': 1475, '48562': 1480, '48896': 1488, '49155': 1496, '49323': 1499, '49938': 1520, '50250': 1527, '50403': 1532, '50535': 1538, '50622': 1541, '50831': 1545, '50953': 1549, '50962': 1551, '51203': 1558, '51662': 1571, '51733': 1575, '51807': 1579, '51959': 1586, '52088': 1590, '52109': 1591, '52149': 1593, '52350': 1602, '52434': 1604, '52608': 1608, '52769': 1614, '52801': 1615, '53673': 1649, '54352': 1669, '54615': 1678, '55106': 1693, '55538': 1706, '55559': 1707, '55643': 1709, '56053': 1723, '56285': 1733, '56393': 1739, '56429': 1740, '56786': 1748, '56880': 1752, '57210': 1760, '57488': 1773, '57618': 1781, '57880': 1795, '58280': 1807, '58581': 1814, '58625': 1817, '58914': 1830, '58900': 1831, '59038': 1836, '59073': 1838, '59230': 1844, '59728': 1855, '59736': 1856, '59796': 1858, '59835': 1859, '60522': 1875, '61448': 1894, '62202': 1914, '62523': 1928, '62575': 1929, '62823': 1941, '63125': 1952, '63315': 1957, '63382': 1960, '63562': 1966, '63693': 1974, '63826': 1979, '63890': 1983, '64002': 1987, '66039': 0, '02128': 1, '00335': 2, '00376': 3, '00689': 10, '00853': 13, '00868': 14, '68450': 15, '01016': 19, '01319': 29, '01373': 30, '65033': 35, '01513': 37, '01598': 41, '01651': 43, '01831': 46, '01874': 47, '69090': 49, '02273': 59, '69210': 66, '65081': 76, '02910': 78, '02969': 80, '03068': 83, '03154': 87, '03201': 88, '03382': 94, '65098': 99, '03756': 105, '03999': 107, '04099': 112, '04226': 121, '04324': 124, '65116': 126, '06120': 132, '04531': 134, '05368': 158, '05778': 170, '05845': 173, '06137': 185, '68008': 188, '06613': 199, '65224': 209, '65246': 227, '07766': 241, '07879': 245, '08114': 250, '08180': 254, '65281': 263, '67455': 266, '67456': 267, '08625': 270, '08635': 271, '08643': 272, '08824': 279, '65302': 287, '70201': 295, '65320': 305, '65322': 307, '69266': 318, '10600': 337, '10820': 346, '69275': 373, '11862': 374, '12107': 385, '12217': 389, '12408': 394, '65393': 395, '12603': 397, '65396': 398, '12649': 400, '65397': 401, '12703': 402, '12809': 404, '12987': 410, '70157': 432, '13740': 436, '14055': 443, '14126': 445, '69286': 446, '14734': 467, '15018': 476, '15110': 479, '15140': 480, '15190': 483, '15632': 497, '70111': 498, '15844': 506, '15900': 508, '16002': 511, '16268': 521, '70374': 527, '16640': 536, '16703': 538, '65501': 545, '65505': 547, '17353': 555, '65522': 556, '17456': 560, '18371': 597, '65606': 599, '18476': 600, '18550': 604, '19183': 620, '67622': 621, '67623': 623, '67627': 630, '65641': 633, '19654': 637, '19742': 640, '19816': 645, '20321': 657, '69314': 666, '20661': 668, '20868': 675, '21086': 683, '67651': 690, '21370': 691, '21820': 699, '22510': 719, '65747': 721, '65757': 729, '22926': 731, '65774': 738, '23414': 741, '23470': 745, '24014': 763, '24127': 767, '24131': 768, '65805': 772, '24329': 773, '24368': 775, '24596': 783, '24995': 793, '25422': 806, '25515': 809, '25871': 818, '26012': 824, '69062': 829, '26591': 839, '27535': 867, '27553': 868, '27848': 877, '28413': 894, '65919': 901, '29278': 912, '65938': 917, '29791': 921, '29842': 922, '65945': 926, '65947': 927, '30217': 931, '30285': 935, '30522': 942, '30881': 951, '30921': 952, '31179': 959, '31346': 965, '65986': 967, '31457': 968, '65990': 972, '67819': 979, '31886': 981, '32197': 988, '66068': 1023, '69388': 1025, '33403': 1031, '67850': 1032, '33651': 1039, '66100': 1061, '35197': 1068, '35574': 1082, '35770': 1085, '35806': 1086, '35923': 1088, '36460': 1103, '36744': 1111, '37003': 1120, '37683': 1142, '66165': 1153, '38299': 1161, '66178': 1165, '66189': 1169, '38973': 1177, '66220': 1183, '66222': 1191, '66227': 1202, '40229': 1214, '40568': 1219, '66256': 1224, '66054': 1225, '67962': 1233, '41078': 1235, '66263': 1236, '41582': 1246, '41653': 1247, '66272': 1250, '42039': 1261, '42214': 1265, '42316': 1267, '70138': 1268, '42541': 1273, '42664': 1280, '42947': 1285, '43087': 1289, '43254': 1294, '43367': 1296, '66309': 1304, '43800': 1309, '43995': 1316, '44007': 1317, '44097': 1321, '44147': 1323, '67096': 1324, '68490': 1329, '44396': 1336, '44538': 1344, '44588': 1348, '44777': 1354, '44790': 1355, '44906': 1361, '45051': 1365, '45090': 1367, '45820': 1395, '46491': 1413, '66377': 1414, '46759': 1420, '46791': 1421, '46775': 1422, '47028': 1427, '67131': 1429, '66389': 1435, '47271': 1437, '47440': 1443, '66394': 1448, '47646': 1452, '66407': 1464, '48273': 1469, '48340': 1473, '48685': 1483, '49005': 1492, '49342': 1501, '49358': 1502, '65577': 1513, '50372': 1531, '50454': 1535, '70307': 1542, '50813': 1544, '50902': 1548, '51160': 1556, '66472': 1561, '51716': 1574, '51749': 1576, '66486': 1582, '69475': 1588, '69476': 1589, '52167': 1594, '52214': 1598, '52380': 1603, '52660': 1611, '52686': 1612, '66521': 1617, '52998': 1620, '70127': 1630, '53261': 1635, '53341': 1637, '53458': 1644, '68750': 1646, '53620': 1647, '53736': 1651, '53771': 1652, '54011': 1658, '54117': 1661, '54363': 1671, '54530': 1675, '54762': 1680, '54788': 1681, '55189': 1696, '67266': 1724, '56271': 1732, '56368': 1738, '67274': 1742, '56763': 1746, '68816': 1747, '57169': 1759, '70056': 1766, '57368': 1770, '57597': 1778, '69026': 1784, '69503': 1786, '57744': 1787, '57890': 1796, '69507': 1802, '58252': 1806, '66649': 1816, '66656': 1828, '58834': 1829, '58972': 1834, '59133': 1841, '59349': 1847, '59383': 1849, '60057': 1866, '60164': 1868, '60730': 1879, '61240': 1891, '66732': 1900, '70171': 1911, '62145': 1912, '62189': 1915, '62229': 1916, '62343': 1922, '62791': 1940, '62942': 1945, '66772': 1947, '63598': 1968, '63701': 1973, '66805': 1978, '66812': 1984, '63970': 1986, '00581': 6, '65014': 12, '00943': 17, '01062': 20, '01073': 21, '65020': 22, '65021': 23, '01224': 26, '01263': 28, '65037': 40, '01621': 42, '02008': 51, '02429': 61, '02484': 63, '02634': 71, '02850': 75, '03089': 84, '03311': 91, '03750': 104, '03797': 106, '04049': 110, '04128': 114, '04135': 115, '04200': 119, '04216': 120, '04300': 123, '04363': 127, '04374': 128, '04971': 146, '05016': 147, '05215': 153, '65149': 157, '05986': 179, '05997': 180, '06077': 184, '06298': 189, '06787': 203, '06922': 205, '06967': 208, '07113': 211, '07126': 212, '07425': 224, '07506': 230, '07523': 231, '07611': 235, '07624': 236, '07699': 238, '07715': 239, '07860': 244, '08087': 249, '08300': 255, '08338': 257, '08510': 265, '08772': 276, '08888': 283, '65301': 286, '09078': 288, '09256': 296, '09500': 302, '10249': 325, '10295': 327, '10388': 330, '10608': 338, '10673': 341, '11103': 353, '11292': 357, '11408': 361, '11418': 362, '11686': 369, '11929': 376, '65382': 378, '11994': 379, '12004': 380, '12022': 382, '12158': 387, '12387': 393, '12636': 399, '12749': 403, '12819': 405, '65398': 406, '12941': 408, '13087': 413, '13106': 414, '13222': 419, '13498': 428, '13507': 429, '65420': 439, '67541': 441, '65425': 442, '14101': 444, '14202': 448, '14390': 454, '14431': 455, '66042': 458, '14601': 461, '14634': 463, '14644': 464, '65438': 465, '14772': 469, '14952': 473, '14960': 474, '14978': 475, '15397': 488, '15447': 490, '15691': 499, '15741': 502, '15874': 507, '15956': 510, '65478': 515, '16211': 519, '16313': 522, '16502': 531, '16540': 533, '65492': 537, '16872': 540, '16949': 543, '69300': 553, '17511': 561, '17529': 562, '17557': 563, '17885': 571, '17958': 576, '17968': 577, '18037': 580, '18069': 582, '18090': 583, '18107': 584, '18640': 605, '18727': 607, '18755': 608, '18854': 611, '18939': 614, '19198': 622, '19297': 627, '19512': 634, '19851': 643, '19929': 644, '19909': 646, '20108': 651, '20256': 656, '20455': 664, '20508': 665, '20670': 669, '20836': 674, '20965': 677, '21524': 696, '21837': 701, '21925': 704, '22015': 706, '65726': 707, '69326': 711, '22356': 714, '65754': 726, '22973': 733, '23029': 736, '23432': 742, '23646': 751, '23959': 761, '24042': 765, '65804': 770, '24343': 774, '65814': 778, '24539': 780, '25270': 801, '25350': 803, '25521': 810, '68502': 815, '26062': 825, '26074': 826, '65877': 834, '26483': 835, '65883': 847, '27017': 852, '27485': 866, '27752': 873, '27915': 879, '28050': 883, '28085': 885, '28285': 890, '28557': 898, '28823': 904, '28811': 905, '65925': 907, '29461': 916, '29897': 924, '65943': 925, '30297': 936, '30582': 944, '65964': 945, '30820': 949, '31072': 956, '31216': 960, '31232': 961, '31297': 963, '31436': 966, '65991': 973, '31687': 975, '32046': 984, '32092': 985, '32275': 991, '32419': 996, '32519': 1001, '32561': 1003, '32575': 1004, '32752': 1009, '32902': 1015, '32999': 1018, '33041': 1019, '33061': 1021, '33517': 1035, '33607': 1037, '33848': 1043, '33880': 1044, '33979': 1045, '34336': 1052, '35306': 1073, '35554': 1080, '35566': 1081, '35673': 1084, '35932': 1089, '35994': 1092, '36036': 1093, '36139': 1096, '36398': 1101, '36487': 1104, '36801': 1113, '36907': 1116, '66148': 1119, '37302': 1129, '37512': 1138, '37644': 1141, '37827': 1144, '37930': 1149, '38206': 1157, '66173': 1164, '38700': 1171, '38944': 1174, '66193': 1176, '39067': 1180, '39074': 1181, '39147': 1184, '39197': 1185, '66221': 1186, '39285': 1187, '39333': 1188, '39386': 1193, '39646': 1200, '40093': 1210, '40094': 1211, '40340': 1216, '40527': 1220, '66258': 1226, '40827': 1230, '41256': 1238, '41559': 1244, '41644': 1248, '41704': 1251, '41771': 1252, '41810': 1253, '41895': 1255, '41902': 1256, '42136': 1263, '42501': 1272, '42601': 1277, '42760': 1283, '43146': 1292, '43507': 1299, '43536': 1301, '66311': 1305, '43873': 1312, '43944': 1315, '44018': 1318, '44166': 1325, '44176': 1326, '44183': 1327, '44296': 1332, '44485': 1340, '44522': 1342, '44530': 1343, '44548': 1345, '44714': 1352, '44810': 1356, '44842': 1358, '44957': 1363, '44969': 1364, '45163': 1370, '45186': 1372, '45218': 1373, '45410': 1377, '45419': 1378, '45467': 1381, '45524': 1384, '45595': 1386, '45636': 1387, '45651': 1388, '45672': 1389, '45829': 1396, '45927': 1401, '46164': 1406, '46310': 1409, '46574': 1415, '46605': 1417, '46656': 1418, '66383': 1424, '47075': 1430, '47092': 1431, '47135': 1433, '47341': 1440, '47478': 1446, '47565': 1449, '47721': 1455, '47802': 1458, '48297': 1470, '48305': 1472, '66417': 1478, '48566': 1481, '48729': 1485, '48933': 1490, '49088': 1494, '49411': 1504, '67166': 1506, '49776': 1516, '49884': 1518, '69459': 1519, '50192': 1526, '50465': 1536, '50578': 1540, '51299': 1562, '51312': 1563, '51771': 1577, '51795': 1580, '66494': 1587, '52114': 1592, '52180': 1596, '52651': 1610, '53035': 1623, '53119': 1625, '53177': 1627, '53196': 1629, '53228': 1631, '53251': 1633, '53401': 1639, '53848': 1654, '53917': 1657, '66549': 1660, '54138': 1662, '54188': 1664, '54248': 1666, '54661': 1679, '54804': 1682, '54829': 1684, '54838': 1685, '55322': 1702, '55626': 1708, '55865': 1718, '55910': 1719, '56113': 1727, '56168': 1729, '56354': 1737, '56546': 1743, '56788': 1749, '57251': 1762, '57344': 1767, '57391': 1768, '57361': 1769, '57542': 1775, '57566': 1776, '57584': 1777, '57604': 1779, '57818': 1793, '58236': 1805, '58663': 1820, '58743': 1824, '58768': 1825, '59000': 1835, '59120': 1840, '59672': 1854, '59839': 1860, '59969': 1864, '69520': 1867, '60338': 1871, '60701': 1878, '60966': 1884, '61130': 1886, '61134': 1887, '61187': 1889, '61219': 1890, '66718': 1892, '69523': 1895, '61587': 1896, '61696': 1897, '61845': 1901, '66734': 1902, '61946': 1905, '62032': 1909, '62384': 1923, '62487': 1927, '62781': 1939, '63044': 1948, '63185': 1953}\n"
     ]
    }
   ],
   "source": [
    "print(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a5ee8f268a58a1501ad7aef09cde53105f57cea18e29cd62af7d0e62261f331"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
